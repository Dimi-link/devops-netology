# Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Основная часть

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информация о сбое: 

* [в виде краткой выжимки на русском языке](https://habr.com/ru/post/427301/);
* [развёрнуто на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).


<table>
<tbody>
<tr>
<td style="text-align: right;">Краткое описание инцидента</td>
<td>Недоступность сервисов GitHub</td>
</tr>
<tr>
<td style="text-align: right;">Время начала инцидента</td>
<td>21.10.2018 22:52 UTC</td>
</tr>
<tr>
<td style="text-align: right;">Время окончания инцидента</td>
<td>22.10.2018 23:03 UTC</td>
</tr>
<tr>
<td style="text-align: right;">Предшествующие события</td>
<td>Плановая замена оптического сетевого модуля 100G в дата-центре</td>
</tr>
<tr>
<td style="text-align: right;">Причина инцидента</td>
<td>Замена неисправного сетевого модуля привела к потере связности между дата-центрами US East Coast и US West Coast, повлекшей за собой нарушение целостности данных в MySQL и проблемам в работе некоторых сервисов</td>
</tr>
<tr>
<td style="text-align: right;">Воздействие</td>
<td>Для выполнения работ по восстановлению были принудительно выключены сервисы notification, pushes и build pages; недоступность репозиториев пользователей</td>
</tr>
<tr>
<td style="text-align: right;">Обнаружение</td>
<td>В 22.54 UTC система мониторинга начала отправлять алерты о многочисленных ошибках в работе системы. Инженерами 
первой линии поддержки в 23.02 UTC было выявлено unexpected-состояние кластеров БД, затем инцидент проэскалирован 
администраторам баз данных</td>
</tr>
<tr>
<td style="text-align: right;">Реакция</td>
<td>Инцидент был устранен в течение 24ч. 11мин.</td>
</tr>
<tr>
<td style="text-align: right;">Восстановление</td>
<td>Произведено восстановление данных MySQL из бэкапа, восстановлена репликация, восстановлена топология репликации, восстановлена работа выключенных сервисов</td>
</tr>
<tr>
<td rowspan="15" style="text-align: right;">Таймлайн&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
<td>2018 October 21 22:52 UTC: запланированная замена неисправного сетевого модуля привела к 43 секундной потере сетевой связности между двумя датацентрами, в которых находились члены MySQL кластеров. Кластера были настроены таким образом, что на запись работали основные ноды в датацентре US East Coast, а остальные ноды, в том числе расположенные в датацентре US West Coast, служили только для чтения. В результате потери связности между датацентрами произошел штатный failover: кластер выбрал нового лидера, была перестроена топология репликации, и запись данных стала осуществляться на ноды, расположенные в датацентре US West Coast.</td>
</tr>
<tr>
<td>2018 October 21 22:54 UTC: появились многочисленные алерты о сбоях в работе. Инженеры первой линии поддержки выявили, что это было вызвано изменившейся топологией кластеров MySQL.</td>
</tr>
<tr>
<td>2018 October 21 23:02 UTC: дежурные инженеры обнаружили что множество кластеров БД оказались в unexpected-состоянии</td>
</tr>
<tr>
<td>2018 October 21 23:07 UTC: кластер был переведен в ручной режим управления топологией. Это повысило приоритет инцидента, к работе подключились владельцы сервиса.</td>
</tr>
<tr>
<td>2018 October 21 23:13 UTC: на этом этапе было обнаружено, что ни один из датацентров не содержит полной реплики данных: новые данные пишутся в датацентр US West Coast уже более получаса, но при этом в датацентре US East Coast содержится некоторое количество записей, которые не успели отреплицироваться из-за нарушения сетевой связности. Дополнительной проблемой являлось то, что из-за изменения топологии базы данных и приложения, их использующие, находятся в разных датацентрах, поэтому возникла дополнительная задержка записи, приводящая к многочисленным ошибкам у пользователей.</td>
</tr>
<tr>
<td>2018 October 21 23:19 UTC: чтобы обеспечить целостность данных, было принято решение об отключении некоторых служб: pages jobs, webhooks, pushes.</td>
</tr>
<tr>
<td>2018 October 22 00:05 UTC: разработан пошаговый план восстановления данных из бэкапа, восстановления топологии кластера и запуска остановленных сервисов.</td>
</tr>
<tr>
<td>2018 October 22 00:41 UTC: начато восстановление из бэкапа. Поскольку данные бэкапа хранились в отдельной локации, параллельно инженеры искали способы ускорить передачу данных.</td>
</tr>
<tr>
<td>2018 October 22 06:51 UTC: включили репликацию свежих данных после завершения восстановления из бэкапа. Значительный объем данных репликации между датацентрами увеличил время отклика на пользовательские запросы. Несогласованность реплик приводила к тому, что пользователи могли получать устаревшие данные.</td>
</tr>
<tr>
<td>2018 October 22 07:46 UTC: GutHub опубликовали пост в блоге, чтобы проинформировать пользователей.</td>
</tr>
<tr>
<td>2018 October 22 11:12 UTC: частично восстановлена правильная топология кластеров, что в общем повысило скорость пользовательских запросов, но так как репликация отдельных кластеров еще не была завершена, то пользователи всё еще получали некорректные ответы. Дополнительной проблемой служило то, что с наступлением дня повысилась пользовательская нагрузка, снизившая скорость восстановления данных.</td>
</tr>
<tr>
<td>2018 October 22 13:15 UTC: пользовательская нагрузка достигла пиковых значений. В совокупности с задачами восстановления данных сервера перестали справляться с нагрузкой, поэтому были в кластера были добавлены новые ноды MySQL на чтение.</td>
</tr>
<tr>
<td>2018 October 22 16:24 UTC: синхронизация реплик завершилась, инициирован возврат к исходной топологии, начата работа по включению notification сервисов, отключенных ранее.</td>
</tr>
<tr>
<td>2018 October 22 16:45 UTC: для восстановления работы notification сервисов была проведена дополнительная работа по балансировке дополнительной нагрузки, а также обновлены значения TTL для тех хуков, у которых истекло время жизни.</td>
</tr>
<tr>
<td>2018 October 22 23:03 UTC: работа веб-хуков и GutHub Pages восстановлена, связность всех систем подтверждена.</td>
</tr>
<tr>
<td rowspan="5" style="text-align: right;">Последующие действия&nbsp;&nbsp;&nbsp;&nbsp;</td>
<td>Собраны бинарные логи с кластеров, уточнено количество неотреплицировавшихся записей и проведена дополнительная работа по их анализу и восстановлению.</td>
</tr>
<tr>
<td>Исправлена конфигурация Orchestrator чтобы предотвратить передачу primary роли между регионами.</td>
</tr>
<tr>
<td>Переработана система оповещения о статусе системы, которая оповещает о статусе отдельных компонентов сервиса.</td>
</tr>
<tr>
<td>Ускорена разработка архитектуры, способной выдерживать падение целого датацентра.</td>
</tr>
<tr>
<td>Улучшена система коммуникации с пользователями в случае инцидентов.</td>
</tr>
</tbody>
</table>
